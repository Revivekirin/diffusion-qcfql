import os
import glob
import json
import random
import time
import tqdm

import numpy as np
import torch

from absl import app, flags
from ml_collections import config_flags

import wandb
from log_utils import setup_wandb, get_exp_name, get_flag_dict, CsvLogger

from envs.env_utils import make_env_and_datasets
from envs.robomimic_utils import is_robomimic_env

from utils.datasets import Dataset 
from evaluation import evaluate

from agents.fql_claude import ACFQLAgent, get_config as get_acfql_config

from torchrl.data import TensorDictReplayBuffer
from torchrl.data.replay_buffers.storages import LazyTensorStorage
from torchrl.data.replay_buffers.samplers import RandomSampler
from tensordict import TensorDict


# ======================================================================
# Flags
# ======================================================================

FLAGS = flags.FLAGS

flags.DEFINE_string('run_group', 'Debug', 'Run group.')
flags.DEFINE_integer('seed', 0, 'Random seed.')
flags.DEFINE_string('env_name', 'transport-mh-vim', 'Environment (dataset) name.')
flags.DEFINE_string('save_dir', 'exp/', 'Save directory.')

flags.DEFINE_integer('offline_steps', 1000000, 'Number of offline steps.')
flags.DEFINE_integer('online_steps', 1000000, 'Number of online steps.')
flags.DEFINE_integer('buffer_size', 2000000, 'Replay buffer size.')
flags.DEFINE_integer('log_interval', 5000, 'Logging interval.')
flags.DEFINE_integer('eval_interval', 100000, 'Evaluation interval.')
flags.DEFINE_integer('save_interval', -1, 'Save interval.')
flags.DEFINE_integer('start_training', 5000, 'when does training start')

flags.DEFINE_integer('utd_ratio', 1, "update to data ratio")
flags.DEFINE_float('discount', 0.99, 'discount factor')

flags.DEFINE_integer('eval_episodes', 50, 'Number of evaluation episodes.')
flags.DEFINE_integer('video_episodes', 0, 'Number of video episodes for each task.')
flags.DEFINE_integer('video_frame_skip', 3, 'Frame skip for videos.')

config_flags.DEFINE_config_file('agent', 'agents/fql.py', lock_config=False)

flags.DEFINE_float('dataset_proportion', 1.0, "Proportion of the dataset to use")
flags.DEFINE_bool('sparse', False, "make the task sparse reward")

flags.DEFINE_bool('save_all_online_states', False, "save all trajectories to npy")

flags.DEFINE_string('entity', 'sophia435256-robros', 'wandb entity')
flags.DEFINE_string('mode', 'online', 'wandb mode')

flags.DEFINE_integer('horizon_length', 5, 'action chunking length.')

# ======================================================================
# Logging helper
# ======================================================================

class LoggingHelper:
    def __init__(self, csv_loggers, wandb_logger):
        self.csv_loggers = csv_loggers
        self.wandb_logger = wandb_logger
        self.first_time = time.time()
        self.last_time = time.time()

    def log(self, data, prefix, step):
        assert prefix in self.csv_loggers, prefix
        self.csv_loggers[prefix].log(data, step=step)
        if self.wandb_logger is not None:
            self.wandb_logger.log({f'{prefix}/{k}': v for k, v in data.items()}, step=step)

# ======================================================================
# Helper: batch numpy -> torch
# ======================================================================

def numpy_batch_to_torch(batch, device):
    torch_batch = {}
    for k, v in batch.items():
        arr = np.asarray(v)

        if arr.dtype == np.bool_:
            t = torch.from_numpy(arr.astype(np.float32))
        elif np.issubdtype(arr.dtype, np.integer):
            t = torch.from_numpy(arr.astype(np.int64))
        else:
            # rewards, masks, terminals, valid, obs, actions ëª¨ë‘ float32ë¡œ í†µì¼
            t = torch.from_numpy(arr.astype(np.float32))

        torch_batch[k] = t.to(device)

    return torch_batch


# ======================================================================
# Main
# ======================================================================

def main(_):
    # Wandb & exp name
    exp_name = get_exp_name(FLAGS.seed)
    run = setup_wandb(project='qc', group=FLAGS.run_group, name=exp_name,
                      entity=FLAGS.entity, mode=FLAGS.mode)

    FLAGS.save_dir = os.path.join(
        FLAGS.save_dir,
        wandb.run.project,
        FLAGS.run_group,
        FLAGS.env_name,
        exp_name,
    )
    os.makedirs(FLAGS.save_dir, exist_ok=True)
    flag_dict = get_flag_dict()

    with open(os.path.join(FLAGS.save_dir, 'flags.json'), 'w') as f:
        json.dump(flag_dict, f)

    # --------- Config (agent) ---------
    # ê¸°ì¡´ì²˜ëŸ¼ FLAGS.agentë¥¼ ì“¸ ìˆ˜ë„ ìˆê³ ,
    # ê·¸ëƒ¥ PyTorchìš© get_acfql_config()ë¡œ dictë¥¼ ê°€ì ¸ì™€ë„ ëœë‹¤.
    # ì—¬ê¸°ì„œëŠ” config_flagsë¡œ ì½ì–´ì˜¨ ë’¤, ê±°ê¸°ì— horizon_lengthë§Œ ë®ì–´ì“´ë‹¤ê³  ê°€ì •.
    # (ì›í•˜ë©´ get_acfql_config()ë¥¼ ì§ì ‘ í˜¸ì¶œí•´ë„ ë¬´ë°©)
    config = dict(get_acfql_config())
    config["horizon_length"] = FLAGS.horizon_length

    # --------- Env & dataset (no OGBench) ---------
    env, eval_env, train_dataset, val_dataset = make_env_and_datasets(FLAGS.env_name)

    # --------- Seed ---------
    random.seed(FLAGS.seed)
    np.random.seed(FLAGS.seed)
    torch.manual_seed(FLAGS.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    log_step = 0
    discount = FLAGS.discount

    # --------- Dataset ì²˜ë¦¬ í•¨ìˆ˜ ---------
    def process_train_dataset(ds):
        """
        - dataset_proportion
        - sparse reward
        - robomimic reward penalty
        """
        ds = Dataset.create(**ds)

        if FLAGS.dataset_proportion < 1.0:
            new_size = int(len(ds['masks']) * FLAGS.dataset_proportion)
            ds = Dataset.create(
                **{k: v[:new_size] for k, v in ds.items()}
            )

        if is_robomimic_env(FLAGS.env_name):
            penalty_rewards = ds["rewards"] - 1.0
            ds_dict = {k: v for k, v in ds.items()}
            ds_dict["rewards"] = penalty_rewards
            ds = Dataset.create(**ds_dict)

        if FLAGS.sparse:
            sparse_rewards = (ds["rewards"] != 0.0) * -1.0
            ds_dict = {k: v for k, v in ds.items()}
            ds_dict["rewards"] = sparse_rewards
            ds = Dataset.create(**ds_dict)

        return ds

    train_dataset = process_train_dataset(train_dataset)
    example_batch = train_dataset.sample(())

    # --------- Agent ìƒì„± (PyTorch) ---------
    agent = ACFQLAgent.create(
        seed=FLAGS.seed,
        ex_observations=np.asarray(example_batch['observations']),
        ex_actions=np.asarray(example_batch['actions']),
        config=config,
        device=str(device),
    )
    
    # --------- Logging setup ---------
    prefixes = ["eval", "env"]
    if FLAGS.offline_steps > 0:
        prefixes.append("offline_agent")
    if FLAGS.online_steps > 0:
        prefixes.append("online_agent")

    logger = LoggingHelper(
        csv_loggers={prefix: CsvLogger(os.path.join(FLAGS.save_dir, f"{prefix}.csv"))
                    for prefix in prefixes},
        wandb_logger=wandb,
    )

    # ==================================================================
    # Offline RL
    # ==================================================================
    offline_init_time = time.time()
    for i in tqdm.tqdm(range(1, FLAGS.offline_steps + 1)):
        log_step += 1

        # (B, H, ...) ì‹œí€€ìŠ¤ ìƒ˜í”Œ
        batch_np = train_dataset.sample_sequence(
            config['batch_size'],
            sequence_length=FLAGS.horizon_length,
            discount=discount,
        )
        batch_torch = numpy_batch_to_torch(batch_np, device=device)

        offline_info = agent.update(batch_torch)  

        if i % FLAGS.log_interval == 0:
            logger.log(offline_info, "offline_agent", step=log_step)

        # saving
        if FLAGS.save_interval > 0 and i % FLAGS.save_interval == 0:
            # TODO: PyTorch ë²„ì „ save_agent êµ¬í˜„ í•„ìš” (state_dict ì €ì¥ ë“±)
            # save_agent(agent, FLAGS.save_dir, log_step)
            pass

        # eval
        if (
            i == FLAGS.offline_steps - 1
            or (FLAGS.eval_interval != 0 and i % FLAGS.eval_interval == 0)
        ):
            eval_info, _, _ = evaluate(
                agent=agent,
                env=eval_env,
                action_dim=example_batch["actions"].shape[-1],
                num_eval_episodes=FLAGS.eval_episodes,
                num_video_episodes=FLAGS.video_episodes,
                video_frame_skip=FLAGS.video_frame_skip,
                debug=True,
            )
            print("[DEBUG] eval info :", eval_info)
            logger.log(eval_info, "eval", step=log_step)

    # ==================================================================
    # Replay Buffer (TorchRL) ì´ˆê¸°í™”
    # ==================================================================
    buffer_size = FLAGS.buffer_size
    batch_size_rb = config["batch_size"] * FLAGS.utd_ratio

    storage = LazyTensorStorage(buffer_size)
    sampler = RandomSampler()

    replay_buffer = TensorDictReplayBuffer(
        storage=storage,
        sampler=sampler,
        batch_size=batch_size_rb,
    )

    # offline datasetìœ¼ë¡œ replay buffer pre-fill
    H = FLAGS.horizon_length
    max_init = min(buffer_size, train_dataset.size)  # ë„ˆë¬´ ì˜¤ë˜ ì•ˆ ëŒê²Œ ì œí•œ

    for _ in range(max_init):
        # (1, H, ...) ì‹œí€€ìŠ¤ í•˜ë‚˜ ë½‘ê¸°
        seq = train_dataset.sample_sequence(
            batch_size=1,
            sequence_length=H,
            discount=discount,
        )

        # ì•„ë˜ shapeëŠ” Dataset êµ¬í˜„ì— ë”°ë¼ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
        # ë³´í†µ:
        #   seq["observations"]      : [1, H, obs_dim]
        #   seq["actions"]           : [1, H, act_dim]
        #   seq["rewards"]           : [1, H]
        #   seq["terminals"]         : [1, H]
        #   seq["masks"]             : [1, H]
        #   seq["next_observations"] : [1, H, obs_dim]

        obs_seq = np.asarray(seq["observations"])[0]        # [H, obs_dim]
        actions_seq = np.asarray(seq["actions"])[0]         # [H, act_dim]
        rewards_seq = np.asarray(seq["rewards"])[0]         # [H]
        terminals_seq = np.asarray(seq["terminals"])[0]     # [H]
        masks_seq = np.asarray(seq["masks"])[0]             # [H]
        next_obs_seq = np.asarray(seq["next_observations"])[0]  # [H, obs_dim]

        # obs0: ì²« íƒ€ì„ìŠ¤í… ê´€ì¸¡
        obs0 = obs_seq[0]   # [obs_dim]

        # valid ë§ˆìŠ¤í¬: ì²« terminal ì´í›„ëŠ” 0
        valid_seq = np.ones(H, dtype=np.float32)
        for t_idx in range(1, H):
            if terminals_seq[t_idx - 1] > 0.5:
                valid_seq[t_idx:] = 0.0
                break

        td = TensorDict(
            {
                "observations": torch.from_numpy(obs0).float(),
                "actions": torch.from_numpy(actions_seq).float(),
                "rewards": torch.from_numpy(rewards_seq).unsqueeze(-1).float(),
                "terminals": torch.from_numpy(terminals_seq).unsqueeze(-1).float(),
                "masks": torch.from_numpy(masks_seq).unsqueeze(-1).float(),
                "next_observations": torch.from_numpy(next_obs_seq).float(),
                "valid": torch.from_numpy(valid_seq).float(),
            },
            batch_size=[],
        )
        replay_buffer.add(td)


    # ==================================================================
    # Online RL
    # ==================================================================
    from collections import defaultdict
    data = defaultdict(list)
    online_init_time = time.time()

    update_info = {}
    for i in tqdm.tqdm(range(1, FLAGS.online_steps + 1)):
        log_step += 1

        # ---------------------------------------------------------------
        # ì•¡ì…˜ ìƒ˜í”Œë§: chunk ë‹¨ìœ„ë¡œ í•œ ë²ˆì— ë½‘ê³  queueì— ìŒ“ê¸°
        # ---------------------------------------------------------------
        if len(action_queue) == 0:
            # obs -> torch
            obs_t = torch.from_numpy(np.asarray(ob)).float().to(device)
            # obs shape [obs_dim]ì´ë©´ ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            if obs_t.dim() == 1:
                obs_t = obs_t.unsqueeze(0)

            with torch.no_grad():
                action_chunk_t = agent.sample_actions(obs_t)
            # [B, H*A] -> numpy
            action_chunk = action_chunk_t.cpu().numpy().reshape(-1, action_dim)

            for a in action_chunk:
                action_queue.append(a)

        action = action_queue.pop(0)

        next_ob, int_reward, terminated, truncated, info = env.step(
            np.clip(action, -1.0 + 1e-5, 1.0 - 1e-5)
        )
        done = terminated or truncated

        # save_all_online_states
        if FLAGS.save_all_online_states:
            state = env.get_state()
            data["steps"].append(i)
            data["obs"].append(np.copy(next_ob))
            data["qpos"].append(np.copy(state["qpos"]))
            data["qvel"].append(np.copy(state["qvel"]))
            if "button_states" in state:
                data["button_states"].append(np.copy(state["button_states"]))

        # env metrics logging
        env_info = {}
        for key, value in info.items():
            if key.startswith("distance"):
                env_info[key] = value
        if len(env_info) > 0:
            logger.log(env_info, "env", step=log_step)

        # reward shaping
        if 'antmaze' in FLAGS.env_name and (
            'diverse' in FLAGS.env_name or 'play' in FLAGS.env_name or 'umaze' in FLAGS.env_name
        ):
            int_reward = int_reward - 1.0
        elif is_robomimic_env(FLAGS.env_name):
            int_reward = int_reward - 1.0

        if FLAGS.sparse:
            assert int_reward <= 0.0
            int_reward = (int_reward != 0.0) * -1.0

        # ---------------------------------------------------------------
        # 1-step transitionì„ windowì— ìŒ“ê¸°
        # ---------------------------------------------------------------
        trans_window.append(
            dict(
                observations=np.asarray(ob, dtype=np.float32),
                actions=np.asarray(action, dtype=np.float32),
                rewards=float(int_reward),
                terminals=float(done),
                masks=float(1.0 - float(terminated)),
                next_observations=np.asarray(next_ob, dtype=np.float32),
            )
        )

        # ---------------------------------------------------------------
        # window ê¸¸ì´ê°€ Hê°€ ë˜ë©´ H-step chunkë¡œ ë§Œë“¤ì–´ ReplayBufferì— ì¶”ê°€
        # ---------------------------------------------------------------
        if len(trans_window) == H:
            obs0 = trans_window[0]["observations"]  # [obs_dim]

            actions_seq = np.stack([t["actions"] for t in trans_window], axis=0)              # [H, act_dim]
            rewards_seq = np.stack([t["rewards"] for t in trans_window], axis=0)              # [H]
            terminals_seq = np.stack([t["terminals"] for t in trans_window], axis=0)          # [H]
            masks_seq = np.stack([t["masks"] for t in trans_window], axis=0)                  # [H]
            next_obs_seq = np.stack([t["next_observations"] for t in trans_window], axis=0)   # [H, obs_dim]

            # valid ë§ˆìŠ¤í¬: ì²« terminal ì´í›„ëŠ” 0
            valid_seq = np.ones(H, dtype=np.float32)
            for t_idx in range(1, H):
                if terminals_seq[t_idx - 1] > 0.5:
                    valid_seq[t_idx:] = 0.0
                    break

            td = TensorDict(
                {
                    "observations": torch.from_numpy(obs0).float(),                     # [obs_dim]
                    "actions": torch.from_numpy(actions_seq).float(),                   # [H, act_dim]
                    "rewards": torch.from_numpy(rewards_seq).unsqueeze(-1).float(),     # [H, 1]
                    "terminals": torch.from_numpy(terminals_seq).unsqueeze(-1).float(), # [H, 1]
                    "masks": torch.from_numpy(masks_seq).unsqueeze(-1).float(),         # [H, 1]
                    "next_observations": torch.from_numpy(next_obs_seq).float(),        # [H, obs_dim]
                    "valid": torch.from_numpy(valid_seq).float(),                       # [H]
                },
                batch_size=[],
            )
            replay_buffer.add(td)

        # ---------------------------------------------------------------
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì²˜ë¦¬
        # ---------------------------------------------------------------
        ob, _ = env.reset()
        action_queue = []
        action_dim = example_batch["actions"].shape[-1]

        # ğŸ”¥ H-step ì‹œí€€ìŠ¤ìš© transition window (ì—í”¼ì†Œë“œ ì „ì²´ì— ê±¸ì³ ìœ ì§€)
        from collections import deque
        H = FLAGS.horizon_length
        trans_window = deque(maxlen=H)
        
        if done:
            ob, _ = env.reset()
            action_queue = []
            trans_window.clear()   # ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘ì´ë¯€ë¡œ window ë¹„ìš°ê¸°
        else:
            ob = next_ob


        # ---------------------------------------------------------------
        # Online í•™ìŠµ
        # ---------------------------------------------------------------
        if i >= FLAGS.start_training and replay_buffer._len >= batch_size_rb:
            batch_td = replay_buffer.sample().to(device)
            # batch_td field shapes:
            #  observations      : [B, obs_dim]
            #  actions           : [B, H, act_dim]
            #  rewards           : [B, H, 1]
            #  terminals         : [B, H, 1]
            #  masks             : [B, H, 1]
            #  next_observations : [B, H, obs_dim]
            #  valid             : [B, H]

            rewards = batch_td["rewards"]
            if rewards.ndim == 3 and rewards.shape[-1] == 1:
                rewards = rewards.squeeze(-1)

            masks = batch_td["masks"]
            if masks.ndim == 3 and masks.shape[-1] == 1:
                masks = masks.squeeze(-1)

            terminals = batch_td["terminals"]
            if terminals.ndim == 3 and terminals.shape[-1] == 1:
                terminals = terminals.squeeze(-1)

            batch = {
                "observations": batch_td["observations"],             # [B, obs_dim]
                "actions": batch_td["actions"],                       # [B, H, act_dim]
                "rewards": rewards,                                   # [B, H]
                "terminals": terminals,                               # [B, H]
                "masks": masks,                                       # [B, H]
                "next_observations": batch_td["next_observations"],   # [B, H, obs_dim]
                "valid": batch_td["valid"],                           # [B, H]
            }

            online_info = agent.update(batch)
            update_info["online_agent"] = online_info


        if i % FLAGS.log_interval == 0:
            for key, info_ in update_info.items():
                logger.log(info_, key, step=log_step)
            update_info = {}

        # eval
        if (
            i == FLAGS.online_steps - 1
            or (FLAGS.eval_interval != 0 and i % FLAGS.eval_interval == 0)
        ):
            eval_info, _, _ = evaluate(
                agent=agent,
                env=eval_env,
                action_dim=action_dim,
                num_eval_episodes=FLAGS.eval_episodes,
                num_video_episodes=FLAGS.video_episodes,
                video_frame_skip=FLAGS.video_frame_skip,
            )
            logger.log(eval_info, "eval", step=log_step)

        # saving
        if FLAGS.save_interval > 0 and i % FLAGS.save_interval == 0:
            agent.save(FLAGS.save_dir)
            # TODO: PyTorch ë²„ì „ save_agent êµ¬í˜„
            # save_agent(agent, FLAGS.save_dir, log_step)
            pass

    end_time = time.time()

    # close csv
    for key, csv_logger in logger.csv_loggers.items():
        csv_logger.close()

    # save_all_online_states
    if FLAGS.save_all_online_states:
        c_data = {
            "steps": np.array(data["steps"]),
            "qpos": np.stack(data["qpos"], axis=0),
            "qvel": np.stack(data["qvel"], axis=0),
            "obs": np.stack(data["obs"], axis=0),
            "offline_time": online_init_time - offline_init_time,
            "online_time": end_time - online_init_time,
        }
        if len(data["button_states"]) != 0:
            c_data["button_states"] = np.stack(data["button_states"], axis=0)
        np.savez(os.path.join(FLAGS.save_dir, "data.npz"), **c_data)

    with open(os.path.join(FLAGS.save_dir, 'token.tk'), 'w') as f:
        f.write(run.url)


if __name__ == '__main__':
    app.run(main)
